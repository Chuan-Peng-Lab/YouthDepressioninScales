# ==========================
# 0. å¯¼å…¥åº“
# ==========================
import re
import jieba
import pandas as pd
import ast
from nltk.util import ngrams
from collections import defaultdict
from gensim import corpora
from gensim.models import LdaModel, CoherenceModel


# ==========================
# 1. åŠ è½½åœç”¨è¯ & ç—‡çŠ¶è¯
# ==========================
def load_wordlist(file_path):
    words = set()
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            w = line.strip()
            if w:
                words.add(w)
    return words


stopwords_file = "hit_stopwords.txt"
symptom_file   = "symptom_keywords.txt"
data_file = "Complains_output_depression_related.xlsx"


stop_words = load_wordlist(stopwords_file)
symptom_keywords = load_wordlist(symptom_file)

print(f"âœ… åœç”¨è¯æ•°é‡ï¼š{len(stop_words)}")
print(f"âœ… ç—‡çŠ¶è¯æ•°é‡ï¼š{len(symptom_keywords)}")


# â­ æ ¸å¿ƒï¼šå¼ºåˆ¶ jieba æŠŠâ€œç—‡çŠ¶çŸ­è¯­â€å½“æˆä¸€ä¸ªæ•´ä½“
for kw in symptom_keywords:
    jieba.add_word(kw)
    jieba.suggest_freq(kw, True)


# ==========================
# 2. è¯»å–ç—…å†æ•°æ®
# ==========================
df = pd.read_excel(data_file)
print(f"âœ… è¯»å–ç—…å† {len(df)} æ¡")


# ==========================
# 3. æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
# ==========================
def preprocess_text(text):
    """
    åŠŸèƒ½ï¼š
    1ï¼‰æ¸…æ´—ç¬¦å·
    2ï¼‰jieba åˆ†è¯ï¼ˆç—‡çŠ¶çŸ­è¯­ä¸è¢«æ‹†ï¼‰
    3ï¼‰ç—‡çŠ¶è¯ä¼˜å…ˆï¼Œå…¶ä½™å†å»åœç”¨è¯
    """
    if pd.isna(text):
        return []

    # æ¸…æ´—éä¸­æ–‡å­—ç¬¦
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', str(text))

    # åˆ†è¯
    raw_tokens = jieba.lcut(text)

    tokens = []
    for t in raw_tokens:
        # â­ è§„åˆ™ 1ï¼šå¦‚æœæ˜¯ç—‡çŠ¶è¯ï¼Œç›´æ¥ä¿ç•™
        if t in symptom_keywords:
            tokens.append(t)

        # â­ è§„åˆ™ 2ï¼šå¦åˆ™ï¼Œæ‰åˆ¤æ–­æ˜¯ä¸æ˜¯åœç”¨è¯
        elif t not in stop_words and len(t) > 1:
            tokens.append(t)

    return tokens



# ==========================
# 4. ä¸»å¤„ç†æµç¨‹ï¼ˆç”Ÿæˆ tokens + bigramsï¼‰
# ==========================
result = defaultdict(dict)

for idx, row in df.iterrows():
    raw_text = row["ç°ç—…å²"]

    tokens = preprocess_text(raw_text)
    bigrams = list(ngrams(tokens, 2))

    # â­ ç—‡çŠ¶å‘½ä¸­ï¼šç›´æ¥åœ¨åŸæ–‡ä¸­æŸ¥çŸ­è¯­ï¼ˆæœ€ç¨³ï¼‰
    hits = [kw for kw in symptom_keywords if kw in str(raw_text)]

    result[idx] = {
        "åŸå§‹æ–‡æœ¬": raw_text,
        "processed_tokens": tokens,
        "bigrams": bigrams,
        "ç—‡çŠ¶å‘½ä¸­": ",".join(hits)
    }

print("âœ… æ–‡æœ¬å¤„ç†å®Œæˆ")


# ==========================
# 5. ä¿å­˜å¤„ç†ç»“æœ
# ==========================
result_df = pd.DataFrame.from_dict(result, orient="index")
save_path = "processed_result_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx"
result_df.to_excel(save_path, index=False)
print(f"âœ… ç»“æœå·²ä¿å­˜ï¼š{save_path}")

# ==========================
# TF-IDFï¼ˆæŒ–æ˜ä¸å¸¸è§ç—‡çŠ¶å€™é€‰ï¼‰
# ==========================

import pandas as pd
import ast
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. è¯»å–ä½ å·²ç»å¤„ç†å¥½çš„ç»“æœæ–‡ä»¶
input_file = "processed_result_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx"
df = pd.read_excel(input_file)

# 2. processed_tokensï¼šlist -> å¥å­ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
def tokens_to_text(x):
    if isinstance(x, str):
        try:
            tokens = ast.literal_eval(x)
            return " ".join(tokens)
        except:
            return ""
    elif isinstance(x, list):
        return " ".join(x)
    else:
        return ""

documents = df["processed_tokens"].apply(tokens_to_text).tolist()

print(f"âœ… TF-IDF è¾“å…¥æ–‡æ¡£æ•°ï¼š{len(documents)}")

# 3. æ„å»º TF-IDF å‘é‡å™¨
vectorizer = TfidfVectorizer(
    min_df=3,        # è‡³å°‘å‡ºç°åœ¨ 3 æ¡ç—…å†ä¸­ï¼ˆé˜²æ­¢çº¯å™ªå£°ï¼‰
    max_df=0.6,      # å‡ºç°å¤ªé¢‘ç¹çš„è¯è‡ªåŠ¨é™æƒ
    token_pattern=r"(?u)\b\w+\b"
)

tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

# 4. è®¡ç®—â€œå…¨è¯­æ–™å¹³å‡ TF-IDFâ€
avg_tfidf = tfidf_matrix.mean(axis=0).A1

tfidf_scores = list(zip(feature_names, avg_tfidf))

# 5. æŒ‰ TF-IDF å¾—åˆ†æ’åº
tfidf_sorted = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)

# 6. å¯¼å‡ºå‰ N ä¸ªå€™é€‰è¯ï¼ˆå»ºè®® 200ï¼‰
TOP_N = 200
top_words = tfidf_sorted[:TOP_N]

output_df = pd.DataFrame(top_words, columns=["å€™é€‰è¯", "å¹³å‡TF-IDF"])

output_file = "TFIDF_top200_å€™é€‰ç—‡çŠ¶.xlsx"
output_df.to_excel(output_file, index=False)

print(f"ğŸ”¥ TF-IDF å®Œæˆï¼Œå·²å¯¼å‡ºï¼š{output_file}")

# ==========================
# 7. Bigram TF-IDFï¼ˆæŒ–æ˜ä¸å¸¸è§ç—‡çŠ¶å€™é€‰ï¼‰
# ==========================

import pandas as pd
import ast
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. è¯»å–å·²å¤„ç†ç»“æœ
input_file = "processed_result_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx"
df = pd.read_excel(input_file)

# 2. processed_tokens â†’ æ–‡æœ¬
def tokens_to_text(x):
    if isinstance(x, str):
        try:
            tokens = ast.literal_eval(x)
            return " ".join(tokens)
        except:
            return ""
    elif isinstance(x, list):
        return " ".join(x)
    else:
        return ""

documents = df["processed_tokens"].apply(tokens_to_text).tolist()

print(f"âœ… TF-IDF è¾“å…¥æ–‡æ¡£æ•°ï¼š{len(documents)}")

# 3. Bigram TF-IDFï¼ˆâ­ åªæ”¹è¿™é‡Œï¼‰
vectorizer = TfidfVectorizer(
    ngram_range=(2, 2),   # â­ åªè¾“å‡ºäºŒå…ƒçŸ­è¯­
    min_df=3,
    max_df=0.6,
    token_pattern=r"(?u)\b\w+\b"
)

tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

# 4. å¹³å‡ TF-IDF
avg_tfidf = tfidf_matrix.mean(axis=0).A1
tfidf_scores = list(zip(feature_names, avg_tfidf))

# 5. æ’åº
tfidf_sorted = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)

# 6. å¯¼å‡ºå‰ 200 ä¸ª
TOP_N = 200
output_df = pd.DataFrame(
    tfidf_sorted[:TOP_N],
    columns=["äºŒå…ƒçŸ­è¯­", "å¹³å‡TF-IDF"]
)

output_file = "TFIDF_bigram_å€™é€‰ç—‡çŠ¶.xlsx"
output_df.to_excel(output_file, index=False)

print(f"ğŸ”¥ Bigram TF-IDF å®Œæˆï¼Œå·²å¯¼å‡ºï¼š{output_file}")
