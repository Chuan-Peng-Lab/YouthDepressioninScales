# ==========================
# 0. å¯¼å…¥åº“
# ==========================
import re
import jieba
import pandas as pd
import ast
from nltk.util import ngrams
from collections import defaultdict
from gensim import corpora
from gensim.models import LdaModel, CoherenceModel


# ==========================
# 1. åŠ è½½åœç”¨è¯ & ç—‡çŠ¶è¯
# ==========================
def load_wordlist(file_path):
    words = set()
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            w = line.strip()
            if w:
                words.add(w)
    return words


stopwords_file = "hit_stopwords.txt"
symptom_file   = "symptom_keywords.txt"
data_file = "Complains_output_non_depression.xlsx"

stop_words = load_wordlist(stopwords_file)
symptom_keywords = load_wordlist(symptom_file)

print(f"âœ… åœç”¨è¯æ•°é‡ï¼š{len(stop_words)}")
print(f"âœ… ç—‡çŠ¶è¯æ•°é‡ï¼š{len(symptom_keywords)}")


# â­ æ ¸å¿ƒï¼šå¼ºåˆ¶ jieba æŠŠâ€œç—‡çŠ¶çŸ­è¯­â€å½“æˆä¸€ä¸ªæ•´ä½“
for kw in symptom_keywords:
    jieba.add_word(kw)
    jieba.suggest_freq(kw, True)


# ==========================
# 2. è¯»å–ç—…å†æ•°æ®
# ==========================
df = pd.read_excel(data_file)
print(f"âœ… è¯»å–ç—…å† {len(df)} æ¡")


# ==========================
# 3. æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
# ==========================
def preprocess_text(text):
    """
    åŠŸèƒ½ï¼š
    1ï¼‰æ¸…æ´—ç¬¦å·
    2ï¼‰jieba åˆ†è¯ï¼ˆç—‡çŠ¶çŸ­è¯­ä¸è¢«æ‹†ï¼‰
    3ï¼‰ç—‡çŠ¶è¯ä¼˜å…ˆï¼Œå…¶ä½™å†å»åœç”¨è¯
    """
    if pd.isna(text):
        return []

    # æ¸…æ´—éä¸­æ–‡å­—ç¬¦
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', str(text))

    # åˆ†è¯
    raw_tokens = jieba.lcut(text)

    tokens = []
    for t in raw_tokens:
        # â­ è§„åˆ™ 1ï¼šå¦‚æœæ˜¯ç—‡çŠ¶è¯ï¼Œç›´æ¥ä¿ç•™
        if t in symptom_keywords:
            tokens.append(t)

        # â­ è§„åˆ™ 2ï¼šå¦åˆ™ï¼Œæ‰åˆ¤æ–­æ˜¯ä¸æ˜¯åœç”¨è¯
        elif t not in stop_words and len(t) > 1:
            tokens.append(t)

    return tokens



# ==========================
# 4. ä¸»å¤„ç†æµç¨‹ï¼ˆç”Ÿæˆ tokens + bigramsï¼‰
# ==========================
result = defaultdict(dict)

for idx, row in df.iterrows():
    raw_text = row["ç°ç—…å²"]

    tokens = preprocess_text(raw_text)
    bigrams = list(ngrams(tokens, 2))

    # â­ ç—‡çŠ¶å‘½ä¸­ï¼šç›´æ¥åœ¨åŸæ–‡ä¸­æŸ¥çŸ­è¯­ï¼ˆæœ€ç¨³ï¼‰
    hits = [kw for kw in symptom_keywords if kw in str(raw_text)]

    result[idx] = {
        "åŸå§‹æ–‡æœ¬": raw_text,
        "processed_tokens": tokens,
        "bigrams": bigrams,
        "ç—‡çŠ¶å‘½ä¸­": ",".join(hits)
    }

print("âœ… æ–‡æœ¬å¤„ç†å®Œæˆ")


# ==========================
# 5. ä¿å­˜å¤„ç†ç»“æœ
# ==========================
result_df = pd.DataFrame.from_dict(result, orient="index")
save_path = "processed_result_éæŠ‘éƒ_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx"
result_df.to_excel(save_path, index=False)
print(f"âœ… ç»“æœå·²ä¿å­˜ï¼š{save_path}")


# ==========================
# 6. HanLP è¯æ€§æ ‡æ³¨ + POS Bigram
# ï¼ˆåŸºäº processed_result_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsxï¼‰
# ==========================

import pandas as pd
import ast
import hanlp

print("\n===== å¼€å§‹è¿›è¡Œ HanLP è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ =====")

# ==========================
# 1ï¸âƒ£ è¯»å–å·²ä¿å­˜çš„åˆ†è¯ç»“æœæ–‡ä»¶
# ==========================
input_file = "processed_result_éæŠ‘éƒ_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx"
result_df = pd.read_excel(input_file)

print(f"âœ… è¯»å–æ–‡ä»¶æˆåŠŸï¼Œå…± {len(result_df)} æ¡æ–‡æœ¬")

# ==========================
# 2ï¸âƒ£ åŠ è½½ HanLP è¯æ€§æ ‡æ³¨æ¨¡å‹
# ==========================
pos_tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN)

# ==========================
# 3ï¸âƒ£ POS æ ‡æ³¨ + POS Bigram
# ==========================
pos_results = []        # [(è¯, è¯æ€§), ...]
pos_bigram_results = [] # [è¯æ€§_è¯æ€§, ...]

for tokens in result_df["processed_tokens"]:

    # Excel é‡Œæ˜¯å­—ç¬¦ä¸²å½¢å¼çš„ listï¼Œéœ€è¦è½¬å› list
    if isinstance(tokens, str):
        try:
            tokens = ast.literal_eval(tokens)
        except:
            tokens = []

    if not tokens:
        pos_results.append([])
        pos_bigram_results.append([])
        continue

    # HanLP è¯æ€§æ ‡æ³¨
    pos_tags = pos_tagger(tokens)

    # è¯ + è¯æ€§
    word_pos = list(zip(tokens, pos_tags))
    pos_results.append(word_pos)

    # POS Bigramï¼ˆä¾‹å¦‚ï¼šn_v, a_nï¼‰
    pos_bigrams = [
        f"{pos_tags[i]}_{pos_tags[i+1]}"
        for i in range(len(pos_tags) - 1)
    ]
    pos_bigram_results.append(pos_bigrams)

# ==========================
# 4ï¸âƒ£ å†™å› DataFrame
# ==========================
result_df["pos_tags"] = pos_results
result_df["pos_bigrams"] = pos_bigram_results

print("âœ… POS æ ‡æ³¨å®Œæˆ")

# ==========================
# 5ï¸âƒ£ å¯¼å‡ºï¼Œæ–¹ä¾¿äººå·¥æŸ¥çœ‹
# ==========================
def list_to_str(x):
    if isinstance(x, list):
        return " | ".join(map(str, x))
    return ""

export_df = result_df.copy()
export_df["processed_tokens"] = export_df["processed_tokens"].apply(list_to_str)
export_df["pos_tags"] = export_df["pos_tags"].apply(list_to_str)
export_df["pos_bigrams"] = export_df["pos_bigrams"].apply(list_to_str)

output_file = "POS_Bigram_éæŠ‘éƒ_ç»“æœæŸ¥çœ‹.xlsx"
export_df.to_excel(output_file, index=False)

print(f"ğŸ“˜ POS + POS Bigram ç»“æœå·²ä¿å­˜ï¼š{output_file}")



# ==========================
# 7. HanLP å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
# ï¼ˆåŸºäº processed_result_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsxï¼‰
# ==========================

import pandas as pd
import ast
import hanlp

print("\n===== å¼€å§‹è¿›è¡Œ HanLP å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ =====")

# ==========================
# 1ï¸âƒ£ è¯»å–å·²å¤„ç†å¥½çš„åˆ†è¯ç»“æœæ–‡ä»¶
# ==========================
input_file = "processed_result_éæŠ‘éƒ_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx"
result_df = pd.read_excel(input_file)

print(f"âœ… è¯»å–æ–‡ä»¶æˆåŠŸï¼Œå…± {len(result_df)} æ¡æ–‡æœ¬")

# ==========================
# 2ï¸âƒ£ åŠ è½½ HanLP NER æ¨¡å‹ï¼ˆå®˜æ–¹é€šç”¨æ¨¡å‹ï¼‰
# ==========================
ner_tagger = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

# ==========================
# 3ï¸âƒ£ å¯¹æ¯æ¡æ–‡æœ¬åš NER
# ==========================
ner_results = []   # æ¯æ¡æ–‡æœ¬çš„å®ä½“ç»“æœ

for tokens in result_df["processed_tokens"]:

    # Excel é‡Œçš„ list æ˜¯å­—ç¬¦ä¸²å½¢å¼
    if isinstance(tokens, str):
        try:
            tokens = ast.literal_eval(tokens)
        except:
            tokens = []

    if not tokens:
        ner_results.append([])
        continue

    # HanLP NERï¼ˆè¾“å…¥ token listï¼‰
    entities = ner_tagger(tokens)

    """
    entities æ ¼å¼ç¤ºä¾‹ï¼š
    [
      ('å­¦æ ¡', 'LOCATION'),
      ('æ¯äº²', 'PERSON'),
      ('åŠå¹´', 'TIME')
    ]
    """
    ner_results.append(entities)

# ==========================
# 4ï¸âƒ£ å†™å› DataFrame
# ==========================
result_df["named_entities"] = ner_results

print("âœ… NER è¯†åˆ«å®Œæˆ")

# ==========================
# 5ï¸âƒ£ å¯¼å‡ºï¼Œæ–¹ä¾¿äººå·¥æŸ¥çœ‹
# ==========================
def list_to_str(x):
    if isinstance(x, list):
        return " | ".join(map(str, x))
    return ""

export_df = result_df.copy()
export_df["processed_tokens"] = export_df["processed_tokens"].apply(list_to_str)
export_df["named_entities"] = export_df["named_entities"].apply(list_to_str)

output_file = "NER_éæŠ‘éƒ_ç»“æœæŸ¥çœ‹.xlsx"
export_df.to_excel(output_file, index=False)

print(f"ğŸ“˜ NER ç»“æœå·²ä¿å­˜ï¼š{output_file}")


# ==========================
# ç»Ÿè®¡å®ä½“é¢‘æ¬¡
# ==========================

import pandas as pd
import re
from collections import Counter

# 1) åªè¯» named_entities è¿™ä¸€åˆ—ï¼ˆæ›´å¿«ï¼‰
input_file = "NER_éæŠ‘éƒ_ç»“æœæŸ¥çœ‹.xlsx"
df = pd.read_excel(input_file, usecols=["named_entities"])

# 2) æ­£åˆ™ï¼šæŠ“å– ('åå’ŒåŒ»é™¢', 'NT', 5, 6) çš„å‰ä¸¤é¡¹
# å…¼å®¹å•å¼•å·/åŒå¼•å·ï¼š('xxx', 'NT'...) æˆ– ("xxx","NT"...)
pattern = re.compile(r"\(\s*['\"]([^'\"]+)['\"]\s*,\s*['\"]([^'\"]+)['\"]")

counter = Counter()

total = len(df)
for i, cell in enumerate(df["named_entities"].fillna("").astype(str), start=1):
    if i % 200 == 0:
        print(f"â³ æ­£åœ¨å¤„ç† {i}/{total} è¡Œ...")

    # ä»å­—ç¬¦ä¸²ä¸­ç›´æ¥æå–æ‰€æœ‰ (å®ä½“å, å®ä½“ç±»å‹)
    matches = pattern.findall(cell)
    if matches:
        counter.update(matches)

# 3) è½¬æˆ DataFrame å¹¶æ’åº
entity_df = pd.DataFrame(
    [(ent, etype, freq) for (ent, etype), freq in counter.items()],
    columns=["å®ä½“", "å®ä½“ç±»å‹", "å‡ºç°æ¬¡æ•°"]
).sort_values(by="å‡ºç°æ¬¡æ•°", ascending=False)

# 4) å¯¼å‡º
output_file = "NER_éæŠ‘éƒ_å®ä½“é¢‘æ¬¡ç»Ÿè®¡.xlsx"
entity_df.to_excel(output_file, index=False)

print(f"âœ… å®Œæˆï¼šå…± {len(entity_df)} ä¸ªå”¯ä¸€å®ä½“-ç±»å‹ç»„åˆ")
print(f"ğŸ“„ å·²å¯¼å‡ºï¼š{output_file}")

entity_df.groupby("å®ä½“ç±»å‹")["å‡ºç°æ¬¡æ•°"].sum().sort_values(ascending=False)


# LDA
import pandas as pd
import ast
from gensim import corpora
from gensim.models import LdaModel, CoherenceModel

# ä»å·²ä¿å­˜çš„åˆ†è¯ç»“æœè¯»å–
lda_df = pd.read_excel("processed_result_éæŠ‘éƒ_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx")

# processed_tokens â†’ list
def parse_list(x):
    if isinstance(x, list):
        return x
    if isinstance(x, str):
        try:
            return ast.literal_eval(x)
        except:
            return []
    return []

lda_df["processed_tokens"] = lda_df["processed_tokens"].apply(parse_list)
lda_df["bigrams"] = lda_df["bigrams"].apply(parse_list)


print("\n===== LDAï¼šå•è¯ç‰ˆï¼ˆbaselineï¼‰=====")

documents = lda_df["processed_tokens"].tolist()

dictionary = corpora.Dictionary(documents)
corpus = [dictionary.doc2bow(doc) for doc in documents]

topics_range = range(2, 10)
coherence_values = []

for k in topics_range:
    lda_tmp = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=k,
        passes=10,
        random_state=42
    )
    cm = CoherenceModel(
        model=lda_tmp,
        texts=documents,
        dictionary=dictionary,
        coherence='c_v',
        processes=1
    )
    coherence_values.append(cm.get_coherence())

best_k = list(topics_range)[coherence_values.index(max(coherence_values))]
print(f"âœ… æœ€ä½³ä¸»é¢˜æ•°ï¼ˆå•è¯ç‰ˆï¼‰ï¼š{best_k}")

lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=best_k,
    passes=10,
    random_state=42
)

for i, t in lda_model.print_topics(num_words=10):
    print(f"ä¸»é¢˜ {i+1}: {t}")

print("\n===== LDAï¼šBigram ç‰ˆï¼ˆç—‡çŠ¶å¢å¼ºï¼‰=====")

documents_bg = []

for bigrams in lda_df["bigrams"]:
    # bigrams = [('å­¦æ ¡','æƒ…ç»ª'), ('æ³¨æ„åŠ›','ä¸é›†ä¸­'), ...]
    bigram_tokens = [f"{a}_{b}" for a, b in bigrams]
    documents_bg.append(bigram_tokens)

dictionary_bg = corpora.Dictionary(documents_bg)
corpus_bg = [dictionary_bg.doc2bow(doc) for doc in documents_bg]

coherence_values_bg = []

for k in topics_range:
    lda_tmp = LdaModel(
        corpus=corpus_bg,
        id2word=dictionary_bg,
        num_topics=k,
        passes=10,
        random_state=42
    )
    cm = CoherenceModel(
        model=lda_tmp,
        texts=documents_bg,
        dictionary=dictionary_bg,
        coherence='c_v',
        processes=1
    )
    coherence_values_bg.append(cm.get_coherence())

best_k_bg = list(topics_range)[coherence_values_bg.index(max(coherence_values_bg))]
print(f"âœ… æœ€ä½³ä¸»é¢˜æ•°ï¼ˆbigramç‰ˆï¼‰ï¼š{best_k_bg}")

lda_model_bg = LdaModel(
    corpus=corpus_bg,
    id2word=dictionary_bg,
    num_topics=best_k_bg,
    passes=10,
    random_state=42
)

for i, t in lda_model_bg.print_topics(num_words=10):
    print(f"ä¸»é¢˜ {i+1}: {t}")




# ==========================
# 7. TF-IDFï¼ˆæŒ–æ˜ä¸å¸¸è§ç—‡çŠ¶å€™é€‰ï¼‰
# ==========================

import pandas as pd
import ast
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. è¯»å–ä½ å·²ç»å¤„ç†å¥½çš„ç»“æœæ–‡ä»¶
input_file = "processed_result_éæŠ‘éƒ_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx"
df = pd.read_excel(input_file)

# 2. processed_tokensï¼šlist -> å¥å­ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
def tokens_to_text(x):
    if isinstance(x, str):
        try:
            tokens = ast.literal_eval(x)
            return " ".join(tokens)
        except:
            return ""
    elif isinstance(x, list):
        return " ".join(x)
    else:
        return ""

documents = df["processed_tokens"].apply(tokens_to_text).tolist()

print(f"âœ… TF-IDF è¾“å…¥æ–‡æ¡£æ•°ï¼š{len(documents)}")

# 3. æ„å»º TF-IDF å‘é‡å™¨
vectorizer = TfidfVectorizer(
    min_df=3,        # è‡³å°‘å‡ºç°åœ¨ 3 æ¡ç—…å†ä¸­ï¼ˆé˜²æ­¢çº¯å™ªå£°ï¼‰
    max_df=0.6,      # å‡ºç°å¤ªé¢‘ç¹çš„è¯è‡ªåŠ¨é™æƒ
    token_pattern=r"(?u)\b\w+\b"
)

tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

# 4. è®¡ç®—â€œå…¨è¯­æ–™å¹³å‡ TF-IDFâ€
avg_tfidf = tfidf_matrix.mean(axis=0).A1

tfidf_scores = list(zip(feature_names, avg_tfidf))

# 5. æŒ‰ TF-IDF å¾—åˆ†æ’åº
tfidf_sorted = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)

# 6. å¯¼å‡ºå‰ N ä¸ªå€™é€‰è¯ï¼ˆå»ºè®® 200ï¼‰
TOP_N = 200
top_words = tfidf_sorted[:TOP_N]

output_df = pd.DataFrame(top_words, columns=["å€™é€‰è¯", "å¹³å‡TF-IDF"])

output_file = "TFIDF_éæŠ‘éƒ_top200_å€™é€‰ç—‡çŠ¶.xlsx"
output_df.to_excel(output_file, index=False)

print(f"ğŸ”¥ TF-IDF å®Œæˆï¼Œå·²å¯¼å‡ºï¼š{output_file}")

# ==========================
# 7. Bigram TF-IDFï¼ˆæŒ–æ˜ä¸å¸¸è§ç—‡çŠ¶å€™é€‰ï¼‰
# ==========================

import pandas as pd
import ast
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. è¯»å–å·²å¤„ç†ç»“æœ
input_file = "processed_result_éæŠ‘éƒ_ç—‡çŠ¶æ–‡æœ¬åˆ†æ.xlsx"
df = pd.read_excel(input_file)

# 2. processed_tokens â†’ æ–‡æœ¬
def tokens_to_text(x):
    if isinstance(x, str):
        try:
            tokens = ast.literal_eval(x)
            return " ".join(tokens)
        except:
            return ""
    elif isinstance(x, list):
        return " ".join(x)
    else:
        return ""

documents = df["processed_tokens"].apply(tokens_to_text).tolist()

print(f"âœ… TF-IDF è¾“å…¥æ–‡æ¡£æ•°ï¼š{len(documents)}")

# 3. Bigram TF-IDFï¼ˆâ­ åªæ”¹è¿™é‡Œï¼‰
vectorizer = TfidfVectorizer(
    ngram_range=(2, 2),   # â­ åªè¾“å‡ºäºŒå…ƒçŸ­è¯­
    min_df=3,
    max_df=0.6,
    token_pattern=r"(?u)\b\w+\b"
)

tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

# 4. å¹³å‡ TF-IDF
avg_tfidf = tfidf_matrix.mean(axis=0).A1
tfidf_scores = list(zip(feature_names, avg_tfidf))

# 5. æ’åº
tfidf_sorted = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)

# 6. å¯¼å‡ºå‰ 200 ä¸ª
TOP_N = 200
output_df = pd.DataFrame(
    tfidf_sorted[:TOP_N],
    columns=["äºŒå…ƒçŸ­è¯­", "å¹³å‡TF-IDF"]
)

output_file = "TFIDF_éæŠ‘éƒ_bigram_å€™é€‰ç—‡çŠ¶.xlsx"
output_df.to_excel(output_file, index=False)

print(f"ğŸ”¥ Bigram TF-IDF å®Œæˆï¼Œå·²å¯¼å‡ºï¼š{output_file}")
